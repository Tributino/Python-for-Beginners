{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lasalle_logo.png\" style=\"width:375px;height:110px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Web Scraping\n",
    "\n",
    "### WIM250 - Introduction to Scripting Languages \n",
    "### Instructor: Ivaldo Tributino\n",
    "\n",
    "Source:\n",
    "    \n",
    "- Automate The Boring Stuff With Python by AL Sweigart\n",
    "- Python for Everybody Exploring Data Using Python 3 by Dr. Charles R. Severance\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- https://realpython.com/beautiful-soup-web-scraper-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "`Web scraping` or `web data extraction` is used for extracting data from websites. The incredible amount of data on the Internet is a rich resource for any field of research or personal interest. To effectively harvest that data, you’ll need to become skilled at `web scraping`. The Python libraries `requests` and `Beautiful Soup` are powerful tools for the job.  \n",
    "\n",
    "<img src=\"images/webScraping.png\" style=\"width:400px;height:300px;\">\n",
    "\n",
    "For example, Google runs many web scraping programs to index web pages for its seach engine. I this class, you will learn about several `modules` that make it easy to scrape web pages in Python.\n",
    "\n",
    "- **webbrowser** Comes with Python and open a browser to a specific page.\n",
    "- **Requests** Python HTTP library that dowloads files and web pages from the internet.\n",
    "- **urllib** is a package that collects several modules for working with URLs.\n",
    "- **Beautiful Soup** Parsing HTML and XML documents. Can be used to extract data from HTML.\n",
    "\n",
    "\n",
    "## 1. webbrowser Module\n",
    "\n",
    "The `webbrowser` module provides a high-level interface to allow displaying Web-based documents to users. Under most circumstances, simply calling the `open()` function from this module will do the right thing.\n",
    "\n",
    "For more information, see: https://docs.python.org/3/library/webbrowser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open('https://www.lasallecollegevancouver.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web browser tab opened the URL `https://www.lasallecollegevancouver.com` This is about the only thing the webbrowser module can do. Even so, the `open()` function does make some interesting things possible. For example,  bring up a map of it on Google Maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad1 = '2665 Renfrew St. Vancouver'\n",
    "ad2 = '4600 Cambie St, Vancouver'\n",
    "ad3 = '1423 E 13th Ave, Vancouver'\n",
    "\n",
    "for ad in [ad1, ad2, ad3]:\n",
    "    webbrowser.open('https://www.google.com/maps/place/%s' %ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Requests Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests module` lets you easily download files from the web without having to worry about complicated issues such as network errors, connection problems, and data compression. The requests module doesn’t come with Python, so you’ll have to install it first.\n",
    "\n",
    "To install libraries in an Anaconda environment, you could try \n",
    "\n",
    "<img src=\"images/install1.png\">\n",
    "\n",
    "or\n",
    "\n",
    "Open the terminal, select the environment and install the library according to the example below:\n",
    "\n",
    "<img src=\"images/install.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://automatetheboringstuff.com/files/rj.txt'\n",
    "webbrowser.open(url)\n",
    "r = requests.get(url) # GET request to retrieve data\n",
    "r.status_code # 200 The request has succeeded, 404 status code for Not Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding # ISO 8859-1 refers to as \"Latin alphabet no. 1\", consisting of 191 characters from the Latin script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type'] # provide information about the request context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plain Text is regular text, with no formatting options such as bold, italics, underlines, or special layout options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = r.text[:251]\n",
    "print(st)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Checking for Errors\n",
    "\n",
    "If you run a cell with the following line:\n",
    "```python\n",
    "res = requests.get('https://www.miktutors.com/page_that_does_not_exist')\n",
    "```\n",
    "You will not receive a `Traceback`. However, this `url` does not exist. Therefore, it is a good idea to use `raise_for_status()` to check the success of the call.\n",
    "\n",
    "<img src=\"images/404.png\" style=\"width:350px;height:225px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.miktutors.com/page_that_does_not_exist') \n",
    "res.status_code # to confirm that the url does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.miktutors.com/page_that_does_not_exist')\n",
    "res.raise_for_status() # returns an HTTP Error object if an error has occurred during the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `raise_for_status()` method is a good way to ensure that a program halts if a bad download occurs. You can wrap the `raise_for_status()` line with `try and except` statements to handle this error case without crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.miktutors.com/page_that_does_not_exist')\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: %s' % (exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Saving download files to the hard drive\n",
    "\n",
    "First, you must open the file in write binary mode by passing the string `'wb'` as the second argument to `open()`. Even if the page is in plaintext (such as the Romeo and Juliet text we will download in the next cell), you need to write binary data instead of text data in order to maintain the Unicode encoding of the text.\n",
    "\n",
    "To write the web page to a file, you can use a for loop with the Response object’s `iter_content()` method.\n",
    "\n",
    "<img src=\"images/savingFiles.png\" style=\"width:200px;height:270px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: %s' % (exc))\n",
    "    \n",
    "    \n",
    "playFile = open('RomeoAndJuliet.txt', 'wb') # \"wb\" mode opens the file in binary format for writing\n",
    "for chunk in res.iter_content(100000): # One hundred thousand bytes at a time\n",
    "        playFile.write(chunk)\n",
    "        \n",
    "playFile.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a file on our hard drive. Let's obtain some information using `Regular Expressions`, learned in the last class, such as get the `Title`, `Author` and `Posting Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular expression\n",
    "fhand = open('RomeoAndJuliet.txt')\n",
    "\n",
    "for line in fhand:\n",
    "    x = re.findall('Posting Date: (.+) ',line) # try 'Author: (.+)' and 'Posting Date: (.+) \\[' \n",
    "    if len(x)>0:\n",
    "        print(x)\n",
    "        break\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to do something more interesting such as discovering which name appears more Romeo or Juliet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times does the name Romeo and Juliet appear?\n",
    "fhand = open('RomeoAndJuliet.txt')\n",
    "dic = {}\n",
    "for line in fhand:\n",
    "    if line.find('Romeo')==-1 and line.find('Juliet')==-1 :\n",
    "        continue\n",
    "    if line.find('Romeo')!=-1: \n",
    "        dic['Romeo'] = dic.get('Romeo',0)+1\n",
    "    if re.search('Juliet',line): \n",
    "        dic['Juliet'] = dic.get('Juliet',0)+1\n",
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the result is surprising. Romeo appears more than a doble compared to Juliet.\n",
    "\n",
    "Okay, but we are not only interested in text files. We would also like to get information in HTML format. Now we need to call the `Beautiful Soup`. To Install it, look for `beautifulsoup4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Beautiful Soup\n",
    "\n",
    "`Beautiful Soup` is a Python library for pulling data out of `HTML` and `XML` files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "\n",
    "Hypertext Markup Language (HTML) is the format that web pages are written in. This course assumes you have some basic experience with HTML, but if you need a beginner tutorial, I suggest the following site:\n",
    "https://htmldog.com/guides/html/beginner/\n",
    "\n",
    "**A Quick Refresher**\n",
    "In case it’s been a while since you’ve looked at any `HTML`, here’s a quick overview of the basics. An `HTML` file is a plaintext file with the .html file extension. The text in these files is surrounded by `tags`, which are words enclosed in `angle brackets`.\n",
    "\n",
    "```\n",
    "<head>\n",
    "  <title>\n",
    "   The Dormouse's story\n",
    "  </title>\n",
    " </head>\n",
    "\n",
    "```\n",
    "\n",
    "There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the `angle brackets`. For example, the `<a>` tag encloses text that should be a link. The URL that the text links to is determined by the `href` attribute. Here’s an example:\n",
    "\n",
    "```\n",
    "Al's free <a href=\"https://inventwithpython.com\">Python books</a>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work on the example using the `Beautiful Soup`:\n",
    "\n",
    "\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HTML used to produce the cell above\n",
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup1 = BeautifulSoup(html_doc, 'html.parser') # parsing text files formatted in HTML\n",
    "\n",
    "type(soup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup1.prettify()) # it will enable us to view how the tags are nested in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extracting all the URLs found within a page’s \\<a> tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The <a> tag defines a hyperlink. The href attribute specifies the URL of the page.\n",
    "for link in soup1.find_all('a'):   #  This code finds all the <a> tags in the document:\n",
    "    print(link.get('href'))        # to get the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 extracting all the text from a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful with this function, we don't want to print anything that is excessively heavy.\n",
    "print(soup1.get_text()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>DON’T USE REGULAR EXPRESSIONS TO PARSE HTML</font>\n",
    "\n",
    "Locating a specific piece of HTML in a string seems like a perfect case for regular expressions. However, I advise you against it. There are many different ways that HTML can be formatted and still be considered valid HTML, but trying to capture all these possible variations in a regular expression can be tedious and error prone. A module developed specifically for parsing HTML, such as bs4, will be less likely to result in bugs.\n",
    "\n",
    "You can find an extended argument for why you shouldn’t parse HTML with regular expressions at https://stackoverflow.com/a/1732454/1893164/.\n",
    "\n",
    "### 3.3 Now let's try to get information directly from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://nostarch.com'\n",
    "res = requests.get(url)        \n",
    "res.raise_for_status()\n",
    "  \n",
    "soup2 = BeautifulSoup(res.text, 'html.parser') \n",
    "type(soup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will see the information in HTML format\n",
    "print(soup2.prettify()[0:1000])    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One common task is extracting all the URLs found within a page’s <link> tags:\n",
    "for link in soup2.find_all('link'):\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the site\n",
    "# MacOS\n",
    "chrome_path = 'open -a /Applications/Google\\ Chrome.app %s'\n",
    "\n",
    "# Windows\n",
    "# chrome_path = 'C:/Program Files (x86)/Google/Chrome/Application/chrome.exe %s'\n",
    "\n",
    "# Linux\n",
    "# chrome_path = '/usr/bin/google-chrome %s'\n",
    "\n",
    "import webbrowser\n",
    "webbrowser.get(chrome_path).open('https://nostarch.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Finding an Element with the `select()` Method\n",
    "\n",
    "You can retrieve a web page element from a `BeautifulSoup` object by calling the `select()`method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: they specify a pattern to look for in this case, in HTML pages instead of general text strings.\n",
    "\n",
    "**CSS** is the acronym of `Cascading Style Sheets`. CSS is a computer language for laying out and structuring web pages (HTML or XML).\n",
    "\n",
    "A full discussion of CSS selector syntax is beyond the scope of this course. However here’s a short introduction to selectors.\n",
    "\n",
    "\n",
    "**Selector passed to the `select()` method // Will match . . .**\n",
    "\n",
    "soup.select('div') ----------------------- All elements named `<div>`\n",
    "    \n",
    "soup.select('#author') ------------------- The element with an id attribute of `author`\n",
    "\n",
    "soup.select('.notice') ------------------- All elements that use a **CSS** class attribute named notice\n",
    "\n",
    "soup.select('div span') ------------------ All elements named `<span>` that are within an element named `<div>`\n",
    "\n",
    "soup.select('div > span') ---------------- All elements named `<span>` that are directly within an element named `<div>`, with no other element in between\n",
    "\n",
    "soup.select('input[name]') --------------- All elements named `<input>` that have a name attribute with any value\n",
    "\n",
    "soup.select('input[type=\"button\"]') ------ All elements named `<input>` that have an attribute named type with value button\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's work on a local file called example.html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "exampleFile = open('example.html')\n",
    "exampleSoup = BeautifulSoup(exampleFile.read(), 'html.parser')\n",
    "elems = exampleSoup.select('#author')\n",
    "print(type(elems)) # elems is a list of Tag objects. <class 'list'>\n",
    "print(len(elems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleFile = open('example.html')\n",
    "a = exampleFile.read()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems[0].get('id') # like {'id': 'author'}.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elems[0].getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exampleSoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now specialists in `select()` method. We will use it on the Lasalle college website to create a list with the words at the footer of the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/footerLinks.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceeded = False\n",
    "try:\n",
    "    resLasalle = requests.get('https://www.lasallecollegevancouver.com')\n",
    "except requests.exceptions.ConnectionError:\n",
    "    exceeded = True\n",
    "    print('Max retries exceeded with URL in requests')\n",
    "    \n",
    "    \n",
    "if exceeded is False:\n",
    "    lasalleSoup = BeautifulSoup(resLasalle.text, 'html.parser')\n",
    "    elems = lasalleSoup.select('#FooterLinksSection')\n",
    "\n",
    "    if len(elems)>0:\n",
    "\n",
    "        footer = elems[0].getText()\n",
    "        print(footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you send many requests from the same IP address in a short period of time, you will likely receive the `'Max retries exceeded with URL in requests'`alert. Then, try the program below.\n",
    "\n",
    "```python\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'https://www.lasallecollegevancouver.com'\n",
    "uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "lasalleSoup = BeautifulSoup(uh, 'html.parser')\n",
    "elems = lasalleSoup.select('#FooterLinksSection')\n",
    "\n",
    "if len(elems)>0:\n",
    "\n",
    "    footer = elems[0].getText()\n",
    "    print(footer)\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dic = {}\n",
    "keys = re.findall('\\n\\n(.+)\\n\\n',footer)\n",
    "for idx in range(len(keys)):\n",
    "    start = footer.find(keys[idx])\n",
    "    try:\n",
    "        end = footer.find(keys[idx+1])\n",
    "    except:\n",
    "        end = len(footer)    \n",
    "    dic[keys[idx]] = re.findall('\\n(.+)',footer[start:end])\n",
    "dic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['About Us']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Using the developer tools to Find HTML elemets\n",
    "\n",
    "What if you’re interested in scraping Corrie Hering background? Right-click where it is on the page (or CONTROL-click on macOS) and select Inspect Element from the context menu that appears. This will bring up the Developer Tools window, which shows you the HTML that produces this particular part of the web page. Figure below shows the developer tools open to the HTML of:\n",
    "\n",
    "```\n",
    "https://www.lasallecollegevancouver.com/about-us/our-professors/corrie-heringa\n",
    "```\n",
    "Note that if the site changes the design of its web pages, you’ll need to repeat this process to inspect the new elements.\n",
    "\n",
    "<img src=\"images/corrie.png\" style=\"width:1000px;height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, time\n",
    "\n",
    "exceeded = False\n",
    "url = 'https://www.lasallecollegevancouver.com/about-us/our-professors/corrie-heringa'\n",
    "try:\n",
    "    resCorrie = requests.get(url)\n",
    "except requests.exceptions.ConnectionError:\n",
    "    exceeded = True\n",
    "    print('Max retries exceeded with URL in requests')\n",
    "    \n",
    "if exceeded is False:\n",
    "    CorrieSoup = bs4.BeautifulSoup(resCorrie.text, 'html.parser')\n",
    "    elems = CorrieSoup.select('[class=\"MainContent\"]')\n",
    "\n",
    "    if len(elems)>0:\n",
    "\n",
    "        text = elems[0].getText()\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Download images from a website\n",
    "\n",
    "Let's fallow a project from the book <i>Automate The Boring Stuff With Python by AL Sweigart</i>. The project is to download each ciomic from the website http://xkcd.com/ . The website has a Prev button that guides the user back trough prior comics. So we need to creare a program does:\n",
    "\n",
    "- Loads the XKCD home page.\n",
    "- Saves the comic images on the page\n",
    "- Follows the Previus Comic link\n",
    "- Repeads until it reaches the first comic.\n",
    "\n",
    "\n",
    "You’ll have a url variable that starts with the value https://xkcd.com and repeatedly update it \n",
    "(in a for loop) At every step in the loop, you’ll download the comic at url. You’ll know to end the loop \n",
    "when url ends with '#'. \n",
    "\n",
    "```python\n",
    "url = 'https://xkcd.com'\n",
    "```\n",
    "You will download the image files to a folder in the current working directory named xkcd. \n",
    "The call `os.makedirs()` ensures that this folder exists, and the exist_ok=True keyword argument prevents \n",
    "the function from throwing an exception if this folder already exists. The remaining code is just \n",
    "comments that outline the rest of your program.\n",
    "\n",
    "```python\n",
    "os.makedirs('xkcd', exist_ok=True) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, bs4\n",
    "\n",
    "url = 'https://xkcd.com'\n",
    "\n",
    "os.makedirs('xkcd', exist_ok=True) \n",
    "\n",
    "while not url.endswith('#'):\n",
    "    \n",
    "    #  Download the page.\n",
    "    print('Downloading page %s...' % url)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()  # to throw an exception and end the program if something went wrong with the download.\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    # Find the URL of the comic image.\n",
    "    \n",
    "    comicElem = soup.select('#comic img') # <img> element inside a <div> element with the id comic\n",
    "    if comicElem == []:\n",
    "        print('Could not find comic image.')\n",
    "    else:\n",
    "        comicUrl = 'https:' + comicElem[0].get('src') # get the src attribute\n",
    "    # Download the image.\n",
    "        print('Downloading image %s...' % (comicUrl))\n",
    "        res = requests.get(comicUrl)\n",
    "        res.raise_for_status()\n",
    "\n",
    "    # TODO: Save the image to ./xkcd.\n",
    "    \n",
    "    imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)),'wb') # naming the images\n",
    "    for chunk in res.iter_content(100000): # writes out chunks of the image data (at most 100,000 bytes each)\n",
    "        imageFile.write(chunk)\n",
    "    imageFile.close()\n",
    "\n",
    "    # TODO: Get the Prev button's url.\n",
    "    prevLink = soup.select('a[rel=\"prev\"]')[0]      # <a> element with the rel attribute set to prev\n",
    "    url = 'https://xkcd.com' + prevLink.get('href') # get the previous comic’s URL\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at Hill Tribe's work, for more information, see https://fusionip.wixsite.com/thitima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser, bs4\n",
    "\n",
    "exceeded = False\n",
    "try:\n",
    "    res = requests.get('https://www.lcieducation.com/en/Portfolios/Students/20697-15868.aspx')\n",
    "except requests.exceptions.ConnectionError:\n",
    "    exceeded = True\n",
    "    print('Max retries exceeded with URL in requests')\n",
    "\n",
    "if exceeded is False:\n",
    "    soup4 = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    images = soup4.find_all('img', {'src':re.compile('.jpg')}) # find_all(name, attrs, recursive, string, limit, **kwargs)\n",
    "    for image in images: \n",
    "        webbrowser.open(image['src']+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have exceeded the maximum number of requests, run the program below using the `urllib.request` library instead of `requests`.\n",
    "\n",
    "for more informations, see; https://docs.python.org/3/library/urllib.request.html\n",
    "\n",
    "```python\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'https://www.lasallecollegevancouver.com/about-us/our-professors/corrie-heringa'\n",
    "uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "CorrieSoup = bs4.BeautifulSoup(uh, 'html.parser')\n",
    "elems = CorrieSoup.select('[class=\"MainContent\"]')\n",
    "\n",
    "if len(elems)>0:\n",
    "\n",
    "    text = elems[0].getText()\n",
    "    print(text)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import urllib.request\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'https://www.lcieducation.com/en/Portfolios/Students/20697-15868.aspx'\n",
    "uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "soup4 = bs4.BeautifulSoup(uh, 'html.parser')\n",
    "images = soup4.find_all('img', {'src':re.compile('.jpg')})\n",
    "for image in images: \n",
    "    webbrowser.open(image['src']+'\\n')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
